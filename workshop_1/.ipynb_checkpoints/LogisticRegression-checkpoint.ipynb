{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logistic_sgd import *\n",
    "import scipy.misc as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose learning parameters.\n",
    "1. learning_rate : magnitude of a step taken with respect to gradient descent\n",
    "2. n_epochs : number of gradient descent steps taken\n",
    "3. batch_size : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "n_epochs=1000\n",
    "batch_size=600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset='mnist.pkl.gz'\n",
    "\n",
    "datasets = load_data(dataset)\n",
    "\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "# compute number of minibatches for training, validation and testing\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we can visualize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = train_set_x.get_value()[0]\n",
    "img = example.reshape((28,28))\n",
    "\n",
    "plt.imshow(img, cmap='Greys_r')\n",
    "plt.title(str(5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build logistic regressor model out of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar representing mini-batch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = T.lscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating image variable and labels\n",
    "\n",
    "1. x : a matrix of images where each row is a 2d image that has been flattened into an array\n",
    "\n",
    "2. y: a vector of labels each element or column is a number from 0-9 representing the true number in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.matrix('x')  # data, presented as rasterized images\n",
    "y = T.ivector('y')  # labels, presented as 1D vector of [int] labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a classifier and a cost tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct the logistic regression class\n",
    "# Each MNIST image has size 28*28\n",
    "classifier = LogisticRegression(input=x, n_in=28 * 28, n_out=10)\n",
    "\n",
    "# the cost we minimize during training is the negative log likelihood of\n",
    "# the model in symbolic format\n",
    "cost = classifier.negative_log_likelihood(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compiling a Theano function that computes the mistakes that are made by\n",
    "# the model on a minibatch\n",
    "test_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=classifier.errors(y),\n",
    "    givens={\n",
    "        x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "        y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "    }\n",
    ")\n",
    "\n",
    "validate_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=classifier.errors(y),\n",
    "    givens={\n",
    "        x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "        y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the gradient of cost with respect to theta = (W,b)\n",
    "g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "# start-snippet-3\n",
    "# specify how to update the parameters of the model as a list of\n",
    "# (variable, update expression) pairs.\n",
    "updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "           (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "# compiling a Theano function `train_model` that returns the cost, but in\n",
    "# the same time updates the parameter of the model based on the rules\n",
    "# defined in `updates`\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify early stopping parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# early-stopping parameters\n",
    "patience = 1000  # look as this many examples regardless\n",
    "patience_increase = 2  # wait this much longer when a new best is\n",
    "                              # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                              # considered significant\n",
    "validation_frequency = min(n_train_batches, patience // 2)\n",
    "                              # go through this many\n",
    "                              # minibatche before checking the network\n",
    "                              # on the validation set; in this case we\n",
    "                              # check every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Train until we have done more than the maximum number of epochs or we have not seen significant decrease\n",
    "in error an our patience has run out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = []\n",
    "losses = []\n",
    "\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "validation_losses = [validate_model(i)\n",
    "                                 for i in range(n_valid_batches)]\n",
    "this_validation_loss = numpy.mean(validation_losses)\n",
    "iterations.append(0)\n",
    "losses.append(this_validation_loss)\n",
    "\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "\n",
    "        minibatch_avg_cost = train_model(minibatch_index)\n",
    "        # iteration number\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            # compute zero-one loss on validation set\n",
    "            validation_losses = [validate_model(i)\n",
    "                                 for i in range(n_valid_batches)]\n",
    "            this_validation_loss = numpy.mean(validation_losses)\n",
    "            iterations.append(iter)\n",
    "            losses.append(this_validation_loss)\n",
    "            \n",
    "            #print(this_validation_loss.shape)\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if this_validation_loss < best_validation_loss *  \\\n",
    "                   improvement_threshold:\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                best_validation_loss = this_validation_loss\n",
    "                # test it on the test set\n",
    "\n",
    "                test_losses = [test_model(i)\n",
    "                               for i in range(n_test_batches)]\n",
    "                test_score = numpy.mean(test_losses)\n",
    "\n",
    "                \n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print(\n",
    "    (\n",
    "        'Optimization complete with best validation score of %f %%,'\n",
    "        'with test performance %f %%'\n",
    "    )\n",
    "    % (best_validation_loss * 100., test_score * 100.)\n",
    ")\n",
    "print('The code run for %d epochs, with %f epochs/sec' % (\n",
    "    epoch, 1. * epoch / (end_time - start_time)))\n",
    "print(('The code ran for %.1fs' % ((end_time - start_time))), file=sys.stderr)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(iterations, losses)\n",
    "plt.title(\"Error loss over iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "predict_model = theano.function(inputs=[classifier.input],outputs=classifier.y_pred)\n",
    "dataset='mnist.pkl.gz'\n",
    "datasets = load_data(dataset)\n",
    "test_set_x, test_set_y = datasets[2]\n",
    "test_set_x = test_set_x.get_value()\n",
    "n = min(n, len(test_set_x))\n",
    "print(test_set_x[:n].shape)\n",
    "predicted_values = predict_model(test_set_x[:n])\n",
    "show_predictions(test_set_x[:n],predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Special Bonus Round !\n",
    "Can you finish this code so that it learns the goal number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goal_num = numpy.array([2],dtype=numpy.int32)\n",
    "\n",
    "goal_image = numpy.empty((1,28*28))##numpy.random.rand(1,28*28)\n",
    "goal_image.fill(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish these two statements so that goal num becomes the correct image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_x = T.grad(cost='Put the correct tensor for cost', \n",
    "             wrt='What tensor do we want to take the gradient with respect to')\n",
    "\n",
    "gradient = theano.function(inputs=[x,y], outputs=g_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gradient(goal_image, goal_num))\n",
    "\n",
    "\n",
    "for i in range(20000):\n",
    "    goal_image -= 0.01 * gradient(goal_image, goal_num)\n",
    "\n",
    "img = goal_image[0].reshape((28,28))\n",
    "\n",
    "plt.imshow(img, cmap='Greys_r')\n",
    "plt.title('learned image')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
